# Magic Leap 和微软的 HoloLens 相比有哪些异同点

> 学习自 : 知乎 2015-12-19 Botao Amber Hu https://www.zhihu.com/question/36921637/answer/70449000

最近看到国内网络上突然 Magic Leap 的话题火了，并且跟着很多人无理由和根据的赞或黑 Magic Leap。我在斯坦福计算机系上学的时候，对 Magic Leap 很好奇，正好在学校能接触到各路和 Magic Leap 相关的大神，所以在这方面做了些研究，我觉得可以分享点技术性干货，解释一些原理，让大家有点材料来赞或黑。

目前 Magic Leap 只有一个公开视频是实际拍摄的： https://www.youtube.com/watch?v=kw0-JRa9n94 （桌腿后的机器人和太阳系）（youku: [Magic Leap Demo](http://v.youku.com/v_show/id_XMTM2NjM0MjE1Ng))，本文只以这个视频的例子来做阐释。

先说一下我关于 Magic Leap 的信息来源：

1. 2014年11月10日，Magic Leap 在2014年9月融了5个亿以后，来 Stanford 招人，开了一个 Info Session，标题是 "The World is Your New Desktop” （世界就是你的新桌面）多么霸气！当时是 Magic Leap 感知研究的高级副总裁 (VP of Perception) Gary Bradski 和 计算视觉的技术负责人 (Lead of Computer Vision) Jean-Yves Bouguet 来作演讲。Gary 是计算机视觉领域的领军人物，在 Intel 和柳树车库(Willow Garage)创造并发展了 OpenCV（计算视觉工具库），也是 ROS（机器人操作系统）的创始团队之一，同时也是 Stanford 顾问教授。Jean-Yves 原来在 Google 负责谷歌街景车（Street View Car）的制造，是计算视觉技术的大牛。他们加入 Magic Leap 是非常令人震惊的。我参加了这次 Info Session, 当时 Gary 来介绍 Magic Leap 在感知部分的技术和简单介绍传说中的数字光场 Cinematic Reality 的原理，并且在允许录影的部分都有拍照记录。本文大部分的干货来自这次演讲。

2. 我今年年初上了 Stanford 计算摄影和数字光场显示的大牛教授 Gordon Wetzstein 的一门课：EE367 Computational Imaging and Display（计算影像和显示器） ：其中第四周的 Computational illumination，Wearable displays 和 Displays Blocks(light field displays) 这三节都讲到 Magic Leap 的原理。现在大家也可以去这个课程网站上看到这些资料，[EE367 / CS448I: Computational Imaging and Display](http://stanford.edu/class/ee367/)

顺便介绍一下 Gordon 所在的 Stanford 计算图形组，由 Marc Levoy（后来跑去造 Google Glass 的大牛教授）一直致力于光场的研究，从 Marc Levoy 提出光场相机，到他的学生 Ren Ng 开创 Lytro 公司制造光场相机，到现在 Gordon 教授制造光场显示器（裸眼光场 3D 显示器），这个组在光场方面的研究一直是世界的领头羊。而Magic Leap可能正在成为光场显示器的最大应用。 [Computational Imaging Research Overview](http://www.computationalimaging.org/research-overview/)

![](https://pic2.zhimg.com/29078586a8217cf6ba49c74b600a24e9_b.png)

3. 今年参加了光场影像技术的研讨会 Workshop on Light Field Imaging ，现场有很多光场技术方面的展示，我和很多光场显示技术的大牛交流了对 Magic Leap 的看法。特别的是，现场体验了接近 Magic Leap 的光场技术 Demo，来自 Nvidia 的 Douglas Lanman 的 Near-Eye Light Field Displays 。[Near-Eye Light Field Displays](http://research.nvidia.com/publication/near-eye-light-field-displays)

4. 今年年中去了微软研究院 Redmond 访问，研究院的首席研究员 Richard Szeliski （计算机视觉大神，计算机视觉课本的作者，Computer Vision: [Algorithms and Applications](http://szeliski.org/book)）让我们试用了 Hololens。感受了 Hololens 牛逼无比的定位感知技术。有保密协议，本文不提供细节，但提供与 Magic Leap 原理性的比较。

下面是干货。

首先呢，科普一下 Magic Leap 和 Hololens 这类 AR 眼镜设备，都是为了让你看到现实中不存在的物体和现实世界融合在一起的图像并与其交互。从技术上讲，可以简单的看成两个部分：

1. 对现实世界的感知 (Perception)
2. 一个头戴式显示器以呈现虚拟的影像 (Display) 。

我会分感知部分和显示部分来分别阐释 Magic Leap 的相关技术。

先简单回答这个问题：

Q1. Hololens 和 Magic Leap 有什么区别？Magic Leap 的本质原理是什么？

感知部分其实 Hololens 和 Magic Leap 从技术方向上没有太大的差异，都是空间感知定位技术。本文之后会着重介绍。Magic Leap 与 Hololens 最大的不同应该来自显示部分，Magic Leap 是用光纤向视网膜直接投射整个数字光场(Digital Lightfield）产生所谓的 Cinematic Reality（电影级的现实）。Hololens 采用一个半透玻璃，从侧面 DLP 投影显示，虚拟物体是总是实的，与市场上 Espon 的眼镜显示器或 Google Glass 方案类似，是个2维显示器，视角还不大，40 度左右，沉浸感会打折扣。

本质的物理原理是：光线在自由空间中的传播，是可以由4维光场唯一表示的。成像平面每个像素中包含到这个像素所有方向的光的信息，对于成像平面来讲方向是二维的，所以光场是4维的。平时成像过程只是对四维光场进行了一个二维积分（每个像素上所有方向的光的信息都叠加到一个像素点上），传统显示器显示这个2维的图像，是有另2维方向信息损失的。而 Magic Leap 是向你的视网膜直接投射整个4维光场， 所以人们通过 Magic Leap 看到的物体和看真实的物体从数学上是没有什么区别的，是没有信息损失的。理论上，使用 Magic Leap 的设备，你是无法区分虚拟物体和现实的物体的。

使用 Magic Leap 的设备，最明显的区别于其他技术的效果是人眼可以直接选择聚焦(主动选择性聚焦)。比如我要看近的物体，近的物体就实，远的就虚。注意这不需要任何的人眼跟踪技术，因为投射的光场还原了所有信息，所以使用者直接可以做到人眼看哪实哪，和真实物体一样。举个例子：在虚拟太阳系视频的 27 秒左右，摄影机失焦了，然后又对上了，这个过程只发生在摄影机里，和 Magic Leap 的设备无关。换句话说，虚拟物体就在那，怎么看是观察者自己的事。这就是 Magic Leap 牛逼的地方，所以 Magic Leap 管自己的效果叫 Cinematic Reality。

![](https://pic4.zhimg.com/6b57a9ae7308bcfcc8869acd0e44b0c3_b.jpg)
![](https://pic3.zhimg.com/245cac1c46be651bd282a528f37e8ba6_b.jpg)

Q2. 主动选择性聚焦有什么好处？传统的虚拟显示技术中，为什么你会头晕？Magic Leap 是怎么解决这个问题的？

众所周知，人类的眼睛感知深度主要是靠两只眼睛和被观察物体做三角定位（双目定位, triangulation cue）来感知被观察物体的与观察者的距离的。但三角定位并不是唯一的人类感知深度的线索，人脑还集成了另一个重要的深度感知线索：人眼对焦引起的物体锐度（虚实）变化（sharpness or focus cue) 。但传统的双目虚拟显示技术（如 Oculus Rift 或 Hololens) 中的物体是没有虚实的。举个例子，如下图，当你看到远处的城堡的时候，近处的虚拟的猫就应该虚了，但传统显示技术中，猫还是实的，所以你的大脑就会引起错乱，以为猫是很远的很大的一个物体。但是这和你的双目定位的结果又不一致，经过几百万年进化的大脑程序一会儿以为猫在近处，一会儿以为猫在远处，来来回回你大脑就要烧了，于是你要吐了。而 Magic Leap 投影了整个光场，所以你可以主动选择性聚焦，这个虚拟的猫就放在了近处，你看它的时候就是实的，你看城堡的时候，它就是虚的，和真实情况一样，所以你不会晕。演讲中 Gary 调侃对于 Jean-Yves 这种带10分钟 Oculus 就吐的家伙来说，现在他一天带16个小时 Magic Leap 都不会晕。谁用谁知道，巴扎嘿！

补充：有人问为什么网上说虚拟现实头晕是因为帧率不够原因？

帧率和延时虽然是目前的主要问题，但都不是太大的问题，也不是导致晕得决定性因素。这些问题用更快的显卡，好的 IMU 和好的屏幕，还有头部动作预测算法都能很好解决。我们要关心一些本质的晕眩问题。

这里要说到虚拟现实和增强现实的不同。

虚拟现实中，使用者是看不到现实世界的，头晕往往是因为人类感知重力和加速度的内耳半规管感受到的运动和视觉看到的运动不匹配导致的。所以虚拟现实的游戏，往往会有晕车想吐的感觉。这个问题的解决不是靠单一设备可以搞定的，如果使用者的确坐在原定不动，如果图像在高速移动，什么装置能骗过你的内耳半规管呢？一些市场上的方案，比如 Omni VR，或者 HTC Vive 这样的带 Tracking 的 VR 系统让你实际行走才解决这个不匹配的问题，但这类系统是受场地限制的。不过 THE VOID 的应用就很好的利用了 VR 的局限，不一定要跑跳，可以用很小的空间做很大的场景，让你以为你在一个大场景里就好了。现在大部分虚拟现实的体验或全景电影都会以比较慢得速度移动视角，否则你就吐了。

但是 Magic Leap 是 AR 增强现实，因为本来就看的到现实世界，所以不存在这个内耳半规管感知不匹配的问题。对于 AR 来讲，主要挑战是在解决眼前投影的物体和现实物体的锐度变化的问题。所以 Magic Leap 给出的解决方案是很好的解决这个问题的。但都是理论上的，至于实际工程能力怎么样就靠时间来证明了。

Q3. 为什么要有头戴式显示器？为什么不能裸眼全息？Magic Leap 是怎么实现的？

人类希望能凭空看到一个虚拟物体，已经想了几百年了。各种科幻电影里也出现了很多在空气中的全息影像。
但其实想想本质就知道，这事从物理上很难实现的：纯空气中没有可以反射或折射光的介质。显示东西最重要的是介质。很多微信上的疯传，以为 Magic Leap 不需要眼镜，我估计是翻译错误导致的，视频中写了 Shot directly through Magic Leap tech.，很多文章错误的翻译成”直接看到”或”裸眼全息"，其实视频是相机透过 Magic Leap 的技术拍的。

目前全息基本还停留在全息胶片的时代（如下图，我在光场研讨会上看到的这个全息胶片的小佛像），或者初音未来演唱会那种用投影阵列向特殊玻璃（只显示某一特定角度的图像，而忽略其他角度的光线）做的伪全息。

![](https://pic4.zhimg.com/87cb1454cfbbe936b67bef9b24bed34b_b.jpg)

![](https://pic2.zhimg.com/a69613a23d44d16fb55ffeb542a83dd1_b.jpg)

![](https://pic2.zhimg.com/e375f5efbf0079c68c025b9d410c77d1_b.jpg)

Magic Leap 想实现的是把整个世界变成你的桌面这样的愿景。所以与其在世界各个地方造初音未来那样的 3D 全息透明屏做介质或弄个全息胶片，还不如直接从人眼入手，直接在眼前投入整个光场更容易。其实 Nvidia 也在做这种光场眼镜，

Nvidia 采用的方法是在一个二维显示器前加上一个微镜头阵列 Microlens array 来生成4维光场。相当于把 2 维的像素映射成4维，自然分辨率不会高，所以这类光场显示器或相机（Lytro) 的分辨率都不会高。本人亲测，效果基本就是在看马赛克画风的图案。

而 Magic Leap 采用完全不同的一个方法实现光场显示，它采用光纤投影。不过，Magic Leap 用的光纤投影的方式也不是什么新东西。在 Magic Leap 做光纤投影显示( Fiber optic projector) 的人是 Brian Schowengerdt ，他的导师是来自华盛顿大学的教授 Eric Seibel，致力于做超高分辨率光纤内窥镜 8 年了。简单原理就是光纤束在一个1mm直径管道内高速旋转，改变旋转的方向，然后就可以扫描一个较大的范围。Magic Leap 的创始人比较聪明的地方，是找到这些做高分辨率光纤扫描仪的，由于光的可逆性，倒过来就能做一个高分辨率投影仪。如图，他们 6 年前的论文，1mm宽9mm长的光纤就能投射几寸大的高清蝴蝶图像。现在的技术估计早就超过那个时候了。

而这样的光纤高分辨率投影仪还不能还原光场，需要在光纤的另一端放上一个微镜头阵列 microlens array，来生成4维光场。你会疑问这不就和 Nvidia 的方法一样了么？不，因为光纤束是扫描性的旋转，这个 microlens array 不用做的很密很大，只要显示扫描到的区域就好了。相当与把大量数据在时间轴上分布开了，和通讯中的分时一样，因为人眼很难分辨 100 帧上的变化，只要扫描帧率够高，人眼就分辨不出显示器是否旋转显示的。所以 Magic Leap 的设备可以很小，分辨率可以很高。

![](https://pic1.zhimg.com/2fff6d0e04ae072ea26e155f7d77d218_b.png)

他本人也来 Stanford 给过一个 Talk，Near-to-Eye Volumetric 3D Displays using Scanned Light。这个 Talk 讲的应该就是 Magic Leap 早期的原型。参考: [Fiber Scanned Displays](http://depts.washington.edu/hplab/research/scanning-displays/)

=== 感知部分 ===

Q4. 首先为什么增强现实要有感知部分？

是因为设备需要知道自己在现实世界的位置（定位），和现实世界的三维结构（地图构建），才能够在显示器中的正确位置摆放上虚拟物体。举个最近的 Magic Leap Demo 视频的例子，比如桌子上有一个虚拟的太阳系，设备佩戴者的头移动得时候，太阳系还呆在原地，这就需要设备实时的知道观看者视角的精确位置和方向，才能反算出应该在什么位置显示图像。同时，可以看到桌面上还有太阳的反光，这就要做到设备知道桌子的三维结构和表面信息，才能正确的投射一个叠加影像在桌子的影像层上。难点是如何做到整个感知部分的实时计算，才能让设备穿戴者感觉不到延时。如果定位有延时，佩戴者会产生晕眩，并且虚拟物体在屏幕上漂移会显得非常的虚假，所谓 Magic Leap 宣称的电影级的真实（Cinematic Reality）就没有意义了。

![](https://pic2.zhimg.com/cca7294ee9601d3139a2606b9e908239_b.jpg)

三维感知部分并不是什么新东西，计算机视觉或机器人学中的 SLAM（Simultaneous Localization And Mapping，即时定位与地图构建）就是做这个的，已经有 30 年的历史了。设备通过各种传感器（激光雷达，光学摄像头，深度摄像头，惯性传感器）的融合将得出设备自己在三位空间中的精确位置，同时又能将周围的三位空间实时重建。

最近 SLAM 技术尤其火爆，去年到今年两年时间内巨头们和风投收购和布局了超级多做空间定位技术的公司。因为目前最牛逼的 3 大科技技术趋势：无人车，虚拟现实，无人机，他们都离不开空间定位。SLAM 是完成这些伟大项目基础中的基础。我也研究 SLAM 技术，所以接触的比较多，为了方便大家了解这个领域，这里简单提几个 SLAM 界最近的大事件和人物:

1. （无人车）Stanford 的机器人教授 Sebastian Thrun 是现代 SLAM 技术的开创者，自从赢了 DARPA Grand Challenge 的无人车大赛后，去了 Google 造无人车了。SLAM 学术圈的大部分研究派系都是 Sebastian 徒子徒孙。
2. (无人车）Uber 在今年拿下了卡耐基梅隆 CMU 的 NREC（国家机器人工程研发中心），合作成立高等技术研发中心 ATC。 这些原来做火星车的定位技术的研究人员都去 Uber ATC 做无人车了。
3. （虚拟现实）最近 Surreal Vision 被 Oculus Rift 收购，其中创始人 Richard Newcombe 是大名鼎鼎的 DTAM，KinectFusion（HoloLens 的核心技术）的发明人。Oculus Rift 还在去年收购了 13th Labs（在手机上做 SLAM  的公司）。
4.（虚拟现实）Google Project Tango 今年发布世界上第一台到手就用的商业化 SLAM 功能的平板。Apple 五月收购 Metaio AR，Metaio AR 的 SLAM 很早就用在了 AR 的 app 上了。Intel 发布 Real Sense，一个可以做 SLAM 的深度摄像头，在 CES 上 Demo 了无人机自动壁障功能和自动巡线功能。
5. （无人机）由原来做 Google X Project Wing 无人机的创始人 MIT 机器人大牛 Nicholas Roy 的学生 Adam Bry 创办的 Skydio，得到 A16z 的两千万估值的投资，挖来了 Georgia Tech 的 SLAM 大牛教授 Frank Dellaert 做他们的首席科学家。http://www.cc.gatech.edu/~dellaert/FrankDellaert/Frank_Dellaert/Frank_Dellaert.html

SLAM 作为一种基础技术，其实全世界做 SLAM 或传感器融合做的好的大牛可能不会多于 100 人，并且大都互相认识。这么多大公司抢这么点人，竞争激烈程度可想而知，所以 Magic Leap 作为一个创业公司一定要融个大资，才能和大公司抢人才资源。

Q5. Magic Leap 的感知部分的技术是怎么样的？

这张照片是 Gary 教授在 Magic Leap Stanford 招聘会中展示了 Magic Leap 在感知部分的技术架构和技术路线。可以看到以 Calibration 为中心，展开成了4支不同的计算机视觉技术栈。

![](https://pic4.zhimg.com/58d3c3a0a9986049ad4f4b04dbde1c47_b.png)

1. 从图上看，整个 Magic Leap 感知部分的核心步骤是 Calibration（图像或传感器校准），因为像 Magic Leap 或 Hololens 这类主动定位的设备，在设备上有各种用于定位的摄像头和传感器， 摄像头的参数和摄像头之间关系参数的校准是开始一切工作的第一步。这步如果摄像头和传感器参数都不准，后面的定位都是无稽之谈。从事过计算机视觉技术的都知道，传统的校验部分相当花时间，需要用摄像头拍摄 Chess Board，一遍一遍的收集校验用的数据。但 Magic Leap 的 Gary，他们发明了一种新的 Calibration 方法，直接用一个形状奇特的结构体做校正器，摄像头看一遍就完成了校正，极为迅速。这个部分现场不让拍照。

2. 有了 Calibration 部分后，开始最重要的三维感知与定位部分（左下角的技术栈），分为4步。

2.1 首先是 Planar Surface Tracking （平面表面跟踪）。大家可以在虚拟太阳系的 Demo 中看到虚拟太阳在桌子上有反光，且这个反光会随着设备佩戴者的移动而改变位置，就像是太阳真的悬在空中发出光源，在桌子表面反射产生的。这就要求设备实时的知道桌子的表面在哪里，并且算出虚拟太阳与平面的关系，才能将太阳的反光的位置算出来，叠在设备佩戴者眼镜相应的位子上，并且深度信息也是正确的。难点在平面检测的实时性和给出平面位置的平滑性（否则反光会有跳变）从 Demo 中可以看出 Magic Leap 在这步上完成的很好。

![](https://pic4.zhimg.com/588f877c639284fab49f6e8f0ac91e17_b.png)

2.2 然后是 Sparse SLAM（稀疏 SLAM）； Gary 在 Info Session 上展示了他们实时的三维重构与定位算法。为了算法的实时性，他们先实现了高速的稀疏或半稀疏的三维定位算法。从效果上看，和目前开源的 LSD 算法差不了太多。

![](https://pic3.zhimg.com/8ca593d2efe8dec4840ea5b605562e9e_b.png)

2.3 接着是 Sensors; Vision and IMU（视觉和惯性传感器融合 ）。

导弹一般是用纯惯性传感器做主动定位，但同样的方法不能用于民用级的低精度惯性传感器，二次积分后一定会漂移。而光靠视觉做主动定位，视觉部分的处理速度不高，且容易被遮档，定位鲁棒性不高。将视觉和惯性传感器融合是最近几年非常流行的做法。

举例：
Google Tango 在这方面就是做 IMU 和深度摄像头的融合，做的很好；大疆的无人机 Phantom 3 或 Inspire 1 将光流单目相机和无人机内的惯性传感器融合，在无 GPS 的情况下，就能达到非常惊人的稳定悬停；Hololens 可以说在 SLAM 方面是的做的相当好，专门定制了一个芯片做 SLAM，算法据说一脉相承了 KinectFusion 的核心，亲自测试感觉定位效果很赞（我可以面对白色无特征的墙壁站和跳，但回到场中心后定位还是很准确的，一点都不飘。）

2.4 最后是 3D Mapping and Dense SLAM （3D地图重建 ）。下图展示了 Magic Leap 山景城办公室的 3D 地图重建：仅仅是带着设备走了一圈，就还原了整个办公室的3D地图，并且有很精致的贴图。书架上的书都能重建的不变形。

![](https://pic1.zhimg.com/547edc1e06c68085bb693b14d08774a4_b.png)

因为 AR 的交互是全新的领域，为了让人能够顺利的和虚拟世界交互，基于机器视觉的识别和跟踪算法成了重中之重。全新人机交互体验部分需要大量的技术储备做支持。

接下来的三个分支，Gary 没有细讲，但是可以看出他们的布局。我就随便加点注解，帮助大家理解。

3.1 Crowdsourcing 众包。用于收集数据，用于之后的机器学习工作，要构建一个合理的反馈学习机制，动态的增量式的收集数据。

3.2 Machine Learning & Deep Learning 机器学习与深度学习。需要搭建机器学习算法架构，用于之后的识别算法的生产。

3.3 Scenic Object Recognition 场景物体识别。识别场景中的物体，分辨物体的种类，和特征，用于做出更好的交互。比如你看到一个小狗的时候，会识别出来，然后系统可以把狗狗p成个狗型怪兽，你就可以直接打怪了。

3.4 Behavior Recognition 行为识别 。识别场景中的人或物的行为，比如跑还是跳，走还是坐，可能用于更加动态的游戏交互。顺便提一下，国内有家 Stanford 校友办的叫格林深瞳的公司也在做这个方面的研究。

跟踪方面

4.1 Gesture Recognition 手势识别。用于交互，其实每个 AR/VR 公司都在做这方面的技术储备。

4.2 Object Tracking 物体追踪。这个技术非常重要，比如 Magic Leap 的手捧大象的 Demo，至少你要知道你的手的三维位置信息，实时 Tracking，才能把大象放到正确的位子。

4.3 3D Scanning 三维扫描。能够将现实物体，虚拟化。比如你拿起一个艺术品，通过三维扫描，远处的用户就能够在虚拟世界分享把玩同样的物体。

4.4 Human Tracking 人体追踪。比如：可以将现实中的每个人物，头上可以加个血条，能力点之类。

5.1 Eye Tracking 眼动跟踪。Gary 解释说，虽然 Magic Leap 的呈像不需要眼动跟踪，但因为要计算4维光场，Magic Leap 的渲染计算量巨大。如果做了眼动跟踪后，就可以减少 3D 引擎的物体渲染和场景渲染的压力，是一个优化的绝佳策略。

5.2 Emotion Recognition 情感识别。如果 Magic Leap 要做一个 Her 电影中描绘的人工智能操作系统，识别主人的情感，可以做出贴心的情感陪护效果。

5.3 Biometrics 生物识别。比如要识别现实场景中的人，在每个人头上显示个名字啥的。人脸识别是其中一种，国内有家清华姚班师兄弟们开得公司 Face++ 就是干这个干的最好的。

总结，简单来讲感知这个部分 Magic Leap 其实和很多其他的公司大同小异，虽然有了 Gary 的加盟，野心非常的宽广，但这部分竞争非常激烈。

Q6: 就算 Magic Leap 已经搞定了感知和显示，那么接下来的困难是什么？

1. 计算设备与计算量。

Magic Leap 要计算 4 维光场，计算量惊人。不知道 Magic Leap 现在是怎么解决的。如果 Nvidia 不给造牛逼的移动显卡怎么办？难道自己造专用电路？背着4块泰坦X上路可不是闹着玩的。

下图是，今年我参加 SIGGraph 2015 里，其中一个 VR 演示，每个人背着个大电脑包玩 VR。10 年后的人类看今天的人类追求 VR 会不会觉得很好笑，哈哈。

![](https://pic4.zhimg.com/e0077f5bd8069c3cf7c9d8c840bf2077_b.png)

2. 电池!电池!电池! 所有电子设备的痛。

3. 一个操作系统。说实话，如果说“世界就是你的新桌面”是他们的愿景，现在的确没有什么操作系统可以支持 Magic Leap 愿景下的交互。他们必须自己发明轮子。

4. 为虚拟物体交互体验增加物理感受。为了能有触感，现在交互手套，交互手柄都是 VR 界大热的话题。从目前的专利上看，并没有看出 Magic Leap 会有更高的见地。说不定某个 Kickstarter 最后能够独领风骚，Magic Leap 再把他收了。

===========

笔者斯坦福计算机系研究生毕业，方向是计算摄影和人工智能，目前在做无人机和虚拟现实技术的研究。

没错，我在招人，简历发 me@botao.hu 带你搞计算摄影，飞行技术和浪天涯。

转载请注明作者：
Botao Amber Hu，现从事无人机和虚拟现实技术的研究，光流科技 C*O，Tech Artist @ Amber Garage。

End.
